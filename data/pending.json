[
  {
    "id": "0cba8eba6b0a",
    "title": "Sudden massive increase in insane hyping of agentic LLMs on twitter",
    "content": "Has anyone noticed this? It's suddenly gotten completely insane. Literally nothing has changed at all in the past few weeks but the levels of bullshit hyping have gone through the roof. It used to be mostly vibesharts that had no idea what they're doing but actual engineers have started yapping complete insanity about running a dozen agents concurrently as an entire development team building production ready complex apps while you sleep with no human in the loop.\n\nIt's as though claude code just came out a week ago and hasn't been more or less the same for months at this point. \n\nWtf is going on",
    "source": "reddit",
    "url": "https://reddit.com/r/ChatGPTCoding/comments/1q49zq0/sudden_massive_increase_in_insane_hyping_of/",
    "author": "u/kidajske",
    "score": 126,
    "date": "2026-01-04",
    "category": "prompt-pattern",
    "metadata": {
      "subreddit": "r/ChatGPTCoding",
      "num_comments": 139
    },
    "scan": {
      "risk_level": "safe",
      "flags": [],
      "llm_analysis": null
    },
    "added_at": "2026-01-10T19:48:17.575450"
  },
  {
    "id": "bd849a0f2c67",
    "title": "Claude Code in RollerCoaster Tycoon",
    "content": "As a Millennial 'digital native' I got a lot of my early intuition for computers from playing video games, and RollerCoaster Tycoon was one of the most computer-y games I played.\n\nAs an adult trying to rebuild my computer intuitions around AI, I wanted to revisit RCT as a study in interfaces, and this transitional moment between Apps, AI; GUIs and CLIs.\n\nThe current AI meta is:\n\n* Just use Claude Code\n* Replace GUIs with CLIs\n\nSo I forked [OpenRCT2](https://openrct2.io/) and vibe coded in a terminal window with Claude Code and a CLI called `rctctl` replicating the game's GUIs for Claude.\n\nIn the Youtube video, the park was pre-built (by a renowned RCT builder), and Claude's task was to identify various problems and fix them, mostly through digital levers, but it also does some construction using just a text-based outputs about the maps and park tiles.\n\n**Extra links:**\n\n[Youtube video](https://www.youtube.com/watch?v=CaFBNIH1gS4)\n\n[Repo/branch](https://github.com/jaysobel/OpenRCT2), if you want to try yourself.\n\n[Session transcript](https://htmlpreview.github.io/?https://gist.githubusercontent.com/jaysobel/dfeed9a65ce7209274acf9ada0eaa65e/raw/claude_code_rollercoaster_tycoon_transcript.html) (using Simon Willison's [claude-code-transcripts](https://github.com/simonw/claude-code-transcripts))",
    "source": "reddit",
    "url": "https://reddit.com/r/ClaudeAI/comments/1q9fen5/claude_code_in_rollercoaster_tycoon/",
    "author": "u/TurtsMcGerts",
    "score": 136,
    "date": "2026-01-10",
    "category": "command",
    "metadata": {
      "subreddit": "r/ClaudeAI",
      "num_comments": 28
    },
    "scan": {
      "risk_level": "warning",
      "flags": [
        "\u26a1 External URL"
      ],
      "llm_analysis": null
    },
    "added_at": "2026-01-10T19:48:17.575950"
  },
  {
    "id": "fd1450535bd9",
    "title": "Open source VS Code extension: Get Copilot-style completions with your existing Claude Max tokens",
    "content": "If you're paying $100-200/month for Claude Max, you probably have unused token capacity. I built a tool to put those tokens to work.\n\n  I love Claude Code CLI for complex, multi-file refactoring and agentic workflows. But sometimes I just want a quick inline completion while typing\u2014and I was tired of paying $19/month for GitHub Copilot on top of my Max subscription.\n\n  So I built Sidekick for Max: a VS Code extension that gives you Copilot-style inline completions powered by your existing Claude Max subscription. No extra cost, no API keys, no separate account.\n\n  How it works:\n  - A local server calls Claude Code CLI to generate completions\n  - Uses Haiku for fast, lightweight inline suggestions (minimal token usage)\n  - Uses Opus for code transforms when you need higher quality refactoring\n  - Your Max subscription handles billing\u2014just authenticate with claude auth\n\n  Features:\n  - Inline completions as you type\n  - Code transforms: select code, press Ctrl+Shift+M, describe what you want\n\n  The token efficiency angle:\n  Most of us aren't hitting our 5-hour usage limits consistently. This puts that unused capacity to work without impacting your CLI workflows. Haiku completions are cheap and fast\u2014you can use them freely throughout the day.\n\n  Links:\n  - https://marketplace.visualstudio.com/items?itemName=CesarAndresLopez.sidekick-for-max\n  - https://github.com/cesarandreslopez/sidekick-for-claude-max\n\n  Would love feedback from other Claude Max users. What features would make this more useful for your workflow?",
    "source": "reddit",
    "url": "https://reddit.com/r/ClaudeAI/comments/1q90kpk/open_source_vs_code_extension_get_copilotstyle/",
    "author": "u/Cal_lop_an",
    "score": 22,
    "date": "2026-01-10",
    "category": "workflow",
    "metadata": {
      "subreddit": "r/ClaudeAI",
      "num_comments": 11
    },
    "scan": {
      "risk_level": "warning",
      "flags": [
        "\u26a1 External URL"
      ],
      "llm_analysis": null
    },
    "added_at": "2026-01-10T19:48:17.577475"
  },
  {
    "id": "a8da2b791929",
    "title": "\u201cYou're 100% right. This is unacceptable.\u201d",
    "content": "Seriously.\n\n$300/mo.\n\n\n\n=== === === === === === === === === === === === \n\n\n\nUPDATE: 8hrs later:\n\nhttps://preview.redd.it/qphk7is7aecg1.png?width=1222&amp;format=png&amp;auto=webp&amp;s=e2ba875938d426873604012666933cad09ea491e\n\nFuck the cynics. Claude is a real one. That's why I started r/ClaudeHomies and now reviving it because I'm so disappointed at how toxic the community in this sub is.\n\nAntrhopic has built not just the best LLM in history, but probably one of the best products in the world - and probably one the best tools humanity have ever had since it's inception.\n\nBut Anthropic is fucking up recently and killing this product, and should be called out. We're not shills, we complain becase we love the product, loyal customers, pay a lot, and want things to be fixed.\n\nIf you think Anthropic can do no wrong, you ain't my homie.",
    "source": "reddit",
    "url": "https://reddit.com/r/Anthropic/comments/1q899d5/youre_100_right_this_is_unacceptable/",
    "author": "u/OptimismNeeded",
    "score": 21,
    "date": "2026-01-09",
    "category": "prompt-pattern",
    "metadata": {
      "subreddit": "r/Anthropic",
      "num_comments": 52
    },
    "scan": {
      "risk_level": "warning",
      "flags": [
        "\u26a1 External URL"
      ],
      "llm_analysis": null
    },
    "added_at": "2026-01-10T19:48:17.592513"
  },
  {
    "id": "1ab56eb4922d",
    "title": "Over christmas break I wrote a fully functional browser with Claude Code in Rust",
    "content": "TL;DR: Saw a tweet about building a privacy-focused browser, built a working engine in 13 days over the holidays. Renders Wikipedia, Twitter, YouTube.\n\nI'm a senior software engineer, 15 years. On December 20, I saw a tweet suggesting someone build \"a browser that doesn't steal your data and has 0 AI features.\"\n\nI had holiday time coming up. I thought \"how hard could it be?\" Is Claude up to the task? Sure seems it was! here is waht I ended up with.\n\n**Total:** \\~50,000 lines of Rust, 13 days\n\n**Tech choices:**\n\n* Rust (memory safety, performance)\n* Boa (JavaScript engine - didn't build my own)\n* wgpu (GPU rendering)\n* DirectWrite (text shaping - platform API)\n* adblock-rust (Brave's engine)\n\n**What I built from scratch with Claude:**\n\n* CSS parser and cascade engine\n* Layout engine (block, inline, flex, grid)\n* DOM implementation\n* HTTP client\n* Image decoders (PNG, JPEG, GIF, WebP)\n\n**What I didn't:**\n\n* JavaScript engine (used Boa)\n* GPU primitives (used wgpu)\n* Platform text rendering (used DirectWrite)\n\n**What works:**\n\n* Wikipedia (complex layout)\n* Twitter (heavy JavaScript SPA)\n* YouTube (video playback)\n* GitHub (code rendering)\n* Most modern websites\n\n**What doesn't:**\n\n* Some CSS edge cases\n* WebGL (planned)\n* Extensions (not planned)\n* Perfect standards compliance\n\n**Demo:** video @hiwavebrowser on x\n\n**Download:**  [ https://github.com/hiwavebrowser/hiwave-windows/releases ](https://github.com/hiwavebrowser/hiwave-windows/releases)   [ https://github.com/hiwavebrowser/hiwave-macos/releases ](https://github.com/hiwavebrowser/hiwave-macos/releases)\n\n**Source:**  [ https://github.com/hiwavebrowser/hiwave ](https://github.com/hiwavebrowser/hiwave)\n\nIt's alpha quality, expect bugs. But it works.\n\nEdit. Update as this was pointed out, I am not attempting to mislead here.. I should have been clearer in my post that:\n\n1. Default mode is hybrid (RustKit + WebView\n2. The \"from scratch\" claim applies to the rendering engine, not the browser chrome and the macOS renderer is currently much farther along than the windows version.\n\nedit2: heres the latest progress on the 100% rustkit renderer, getting somewhere [smoke testing](https://imgur.com/a/iZ87mR2)\n\nedit3. I've been able to improve baseline render pixel parity (compared to a regular browser) progress is slow but steady. Here is a video from [two hours ago](https://imgur.com/a/AiYe0FE).  Here is a video from [a moment ago](https://imgur.com/a/RJW95ht). I'm now working to collect these tests and compare changes against each other to see what nobs are causing what results to try and make some more careful improvements. It always ends up some 0 size field coming from somewhere else.\n\nEdit 4 1-7 Noon: Sorry I didn\u2019t realize the second video was cutoff at the bottom, it was about 40% parity match. Overnight I accomplished \\~60%! This was using my swarm testing. Im likely to burn out my cursor queue this weekend if not before. 68% right now, so I\u2019m gonna focus more on getting the Linux build up and running. My Linux laptop is having some issues recognizing its own hardware so I\u2019m going to update to the latest Ubuntu in a Hail Mary fix. I\u2019m waiting on the Amazon man to deliver a thumb drive big enough to hold the iso (it\u2019s gotten pretty large)\n\nEdit 5: Renderer reporting 88%, unsure, definite improvements and this value is an average so its possible, but still a ways to go. Will have to take a look at how we compute the values and revisit. I'd say 65% is maybe closer but the way its not linear I guess things are possible. [Video Proof](https://imgur.com/a/4nuThsP) Dont let the blurryness of the video fool you, its actually getting good. Have a headache tonight so progress is scant today.\n\nEdit 6. Final for this post. I\u2019ll make a new post Monday sometime. I just added the Linux repo a moment ago. I\u2019ve also added nightly and weekly builds for each target, they may or may not work. Will fix errors as they arise. The macOS build has made considerable progress on the renderer, 100% pixel parity (or very close to it) however pages aren\u2019t fully displaying properly, taking a new approach. I\u2019ve also added some nightly parity tests to make sure we don\u2019t break things. No videos today, I\u2019m exhausted from the week, but not giving up, still very much committed to this idea.",
    "source": "reddit",
    "url": "https://reddit.com/r/Anthropic/comments/1q4xfm0/over_christmas_break_i_wrote_a_fully_functional/",
    "author": "u/Han_Thot_Terse",
    "score": 117,
    "date": "2026-01-05",
    "category": "command",
    "metadata": {
      "subreddit": "r/Anthropic",
      "num_comments": 71
    },
    "scan": {
      "risk_level": "warning",
      "flags": [
        "\u26a1 External URL"
      ],
      "llm_analysis": null
    },
    "added_at": "2026-01-10T19:48:17.592873"
  },
  {
    "id": "59ccedc28482",
    "title": "Custom Agent removal has completely broken my trust in Cursor",
    "content": "EDIT: Switching to Claude Code. No better time to learn a new workflow than when Cursor broke yours\n\nMy entire Cursor workflow relied on Custom Agents. I come back from vacation to see them missing, only to learn they've been [completely and intentionally removed](https://forum.cursor.com/t/return-the-custom-modes-features/144170/27).\n\nI received absolutely no warning of this and have lost tons of valuable workflows because Cursor thought they knew what was best for me. Cursor has continuously taken actions that break trust in my ability to use their product (all while slowly ramping up cost).\n\nI guess walking in to discover your workflows have all been deleted is a good time to move to another product.",
    "source": "reddit",
    "url": "https://reddit.com/r/cursor/comments/1q6lgz2/custom_agent_removal_has_completely_broken_my/",
    "author": "u/PreviousLadder7795",
    "score": 25,
    "date": "2026-01-07",
    "category": "workflow",
    "metadata": {
      "subreddit": "r/cursor",
      "num_comments": 9
    },
    "scan": {
      "risk_level": "warning",
      "flags": [
        "\u26a1 External URL"
      ],
      "llm_analysis": null
    },
    "added_at": "2026-01-10T19:48:17.593435"
  },
  {
    "id": "3127a173af5a",
    "title": "Opus 4.5 head-to-head against Codex 5.2 xhigh on a real task. Neither won.",
    "content": "I'm home alone after New Years. What do I decide to do? Force my two favorite AI coding \"friends\" to go head-to-head.\n\nI expected to find a winner. Instead, I found something more interesting: using both models together was more effective than using either individually.\n\n# The Setup\n\nThis wasn't benchmarks or \"build Minecraft from scratch.\" This was real work: adding vector search to my AI dev tooling (an MCP server I use for longer-term memory).\n\n**The rules**: SOTA models, same starting prompt, parallel terminals.\u00a0**The tools**: Anthropic $100/m subscription, ChatGPT Plus (~~$20~~\u00a0$0/m for this month -\u00a0*thanks Sam!*)\n\nBoth models got the same task across three phases:\n\n* **Research**\u00a0\\- Gather background, find relevant code\n* **Planning**\u00a0\\- Create a concrete implementation plan\n* **Review**\u00a0\\- Critique each other's plans\n\nI've used Claude pretty much daily since April. I've used Codex for three days. My workflow was built around Claude's patterns. So there's definitely a Claude bias here - but that's exactly what makes the results interesting.\n\n# The Highlights\n\n**Research phase:**\u00a0Claude recommended Voyage AI for embeddings because they're an \"Anthropic partner.\" I laughed out loud. Claude citing its creator's business partnerships as a technical justification is either endearing or concerning - especially given the flak OpenAI gets for planned ads. Turns out Anthropic may have beat them to it...\n\n**Planning phase:**\u00a0Claude produces cleaner markdown with actionable code snippets. Codex produces XML-based architecture docs. Different approaches, both reasonable.\n\n**Review phase:**\u00a0This is where it got interesting.\n\nI asked each model to critique both plans (without telling them who wrote which). Round 1 went as expected\u2014each model preferred its own plan.\n\nThen Codex dropped this:\n\n&gt;\n\nAt first look Claude's plan was reasonable to me - it looked clean, well-structured, thoroughly reasoned. It also contained bugs / contradictions.\n\nCodex found two more issues:\n\n* Claude specified both \"hard-fail on missing credentials\" AND \"graceful fallback\"\u2014contradictory\n* A tool naming collision with an existing tool\n\nWhen I showed Claude what Codex found:\n\n&gt;\n\nThe plan was better off by having a second pair of eyes.\n\n# My Takeaway\n\nThe winner isn't Codex or Claude - it's running both.\n\nFor daily coding, I've switched to Codex as my primary driver. It felt more adherent to instructions and more thorough (plus the novelty is energizing). Additionally, when compared to Codex, Claude seemed a bit... ditzy. I never noticed it when using Claude alone, but compared to Codex, the difference was noticeable.\n\nFor anything that matters (architecture decisions, complex integrations), I now run it past both models before implementing.\n\nThe $200/month question isn't \"which model is best?\" It's \"when is a second opinion worth the overhead?\" For me: any time I find myself wondering if the wool is being pulled over my eyes by a robot (which it turns out is pretty often).\n\nSorry Anthropic, you lost the daily driver slot for now (try again next month!). But Claude's still on the team.\n\n# The Receipts\n\nI documented everything. Full transcripts, the actual plans, side-by-side comparisons. If you want to see exactly what happened (or disagree with my conclusions), the raw materials are on my blog:\u00a0[https://benr.build/blog/claude-vs-codex-messy-middle](https://benr.build/blog/claude-vs-codex-messy-middle)\n\nThis is n=1. But it's a documented n=1 with receipts, which is more than most AI comparisons offer.\n\nCurious if anyone else has tried running multiple models on the same task. What patterns have you noticed?",
    "source": "reddit",
    "url": "https://reddit.com/r/ChatGPTCoding/comments/1q6m1ui/opus_45_headtohead_against_codex_52_xhigh_on_a/",
    "author": "u/bisonbear2",
    "score": 48,
    "date": "2026-01-07",
    "category": "workflow",
    "metadata": {
      "subreddit": "r/ChatGPTCoding",
      "num_comments": 25
    },
    "scan": {
      "risk_level": "warning",
      "flags": [
        "\u26a1 External URL"
      ],
      "llm_analysis": null
    },
    "added_at": "2026-01-10T19:48:17.594185"
  },
  {
    "id": "ce947511244d",
    "title": "I built a \"Forest of Trees\" interface for LLMs because linear chats (like ChatGPT) were getting too messy for my study sessions.",
    "content": "Hi r/ChatGPTCoding\n\nMy name is Farrell, I want to share about the tool I made to solve a frustration I have when using ChatGPT\n\n\u00a0I like using LLMs to study, and whenever I do, I have a\u00a0*lot*\u00a0of side questions.\n\n* \"Wait, what does that term mean?\"\n* \"Can you explain that specific concept?\"\n\nIn ChatGPT (*or Gemini, Claude, etc*), whenever I ask these side questions, it clutters the main thread. Often times, I even ask follow up questions TO my side questions. So I either have to scroll to where I left off, `Ctrl+F` , or start a new chat and lose all the context. ChatGPT has folders and their own branching feature, but it doesn't feel enough for me.\n\nSo I built\u00a0**Divi**. It works like a \"forest of trees\" or \"fractal\". Whenever you have a side question, you just\u00a0**Branch**\u00a0off. Branching gives you a clean chatroom, while remembering the full conversation you had earlier (from the parent branch).\n\nOnce you\u2019re satisfied with the answer, you can jump back to your original chat like nothing happened. You can branch as much as you want, even from other branches.\n\nThat's the gist of Divi, I originally intended it as a personal tool, but having heard my friends asking to try it out, I tried making it online. See if it might help improve your \"Quality of Life\" when using chatGPT. It's on: [https://trydivi.vercel.app/](https://trydivi.vercel.app/) (I'll remove the link if it violates the subreddit rules \ud83d\ude4f\ud83c\udffb)\n\nHere are more explanation on the other features I made:\n\nObviously, creating infinite branches can get confusing. To fix this, I added a\u00a0**Graph Visualization**\u00a0(inspired by Obsidian). Every branch is a node, so you can visually see your \"train of thought\" and jump between conversations just by clicking on the graph.\n\n**How \"Context\" Works in Divi:** Unlike standard chats where context is just \"the last 50 messages,\" Divi treats context like a family tree.\n\n* **Vertical Inheritance:**\u00a0When you reply to a message, the AI sees the full history of that \"Branch\" all the way back to the \"Root.\"\n* **Horizontal Isolation:**\u00a0Sibling branches are invisible to each other. If you branch off to talk about\u00a0*Topic A*, and then make a new branch for\u00a0*Topic B*, the AI in Branch B knows nothing about Branch A. This keeps your clean slate truly clean.\n\nAnd I added a few things I personally needed,\n\n**1. The Global Backpack (Topic-Specific Memory):**\u00a0This sits at the \"Root\" of your tree. Any text you put here is injected into\u00a0*every single branch*\u00a0of that specific topic.\n\n* You start a \"Cooking Research\" chat, you put \"I am allergic to peanuts\" in the Global Backpack. Now, whether you are 5 branches deep looking at Thai food or Italian food, the AI knows \"you are allergic to peanuts\".\n* This memory\u00a0stays\u00a0in the Cooking chat. If you start a new Root for \"Studying LLMs,\" the AI won't know that you \"have a peanut allergy. It isolates memory per topic.\n\n**2. The Universal Backpack:**\u00a0Since the Global Backpack\u00a0*only*\u00a0lives inside its own topic, I needed a place for instructions I want every single AI to know. You can head to\u00a0**Settings -&gt; Universal Memory**\u00a0to add instructions that apply globally across\u00a0*all*\u00a0your roots and branches. This is perfect for instructions like \"Always explain things like I'm 5,\" or \"Code in Python unless specified.\" It ensures your preferences are respected everywhere, not just in one specific chat.\n\n**3. Model Switching:**\u00a0You can switch models at any point of the conversation while retaining all the chat history.\n\nDivi free to use, but since I'm paying for the API calls out of pocket, I require Google Sign-In to prevent bot attacks and huge cloud bills.\n\n* **Free Tier:**\u00a0100k tokens/month (enough for casual use).\n* **For Testers:**\u00a0If anyone here actually finds this useful and hits the limit, just DM me or leave feedback. I\u2019d be happy to upgrade you to the 3M token tier for free in exchange for feedbacks!\n\nI\u2019d really appreciate any feedback to help improve it. Thanks!\n\nTry Divi: [https://trydivi.vercel.app/](https://trydivi.vercel.app/)\n\nP.S (I used AI a bit to help structure my words a bit since I suck and tend to overexplain things. I hope that's okay here.)",
    "source": "reddit",
    "url": "https://reddit.com/r/ChatGPTCoding/comments/1q3d0ve/i_built_a_forest_of_trees_interface_for_llms/",
    "author": "u/BonkNotSus",
    "score": 42,
    "date": "2026-01-03",
    "category": "command",
    "metadata": {
      "subreddit": "r/ChatGPTCoding",
      "num_comments": 25
    },
    "scan": {
      "risk_level": "warning",
      "flags": [
        "\u26a1 External URL"
      ],
      "llm_analysis": null
    },
    "added_at": "2026-01-10T19:48:17.594842"
  },
  {
    "id": "0e6d3956a1c3",
    "title": "Some tips for other newbs like me",
    "content": "**Disclaimer**: I'm on the 5x plan, and I almost exclusively use Opus 4.5 in Claude Code CLI (unless I'm \"writing\" copy, then Sonnet 4.5)\n\nI was burning through consumption on the Pro plan and decided to upgrade to 5x.  I hit usage limits a lot less now, but I still try to be as token-efficient as possible. I work on 3 different projects simultaneously, after all.  So - instead of just entering in basic prompts like \"fix this bug: ... \" or \"add this feature: ...\" I upped my game a bit.  \n\nHere's some strategies that have worked for me, boosting my own productivity and preventing a) undesirable bugs from surfacing and b) creating more token efficiency to help me burn through less utilization. \n\n**Use /plan** before every \\[decent-sized\\] bug fix and feature add.  When asking for a plan with /plan, specify the following: \"in your plan, detail implementation steps that you could address in chunks, without having prior context fresh in memory to address the subsequent chunk.\" (I'll explain this more down below)\n\n**Run /clear after every task completion and plan creation.**  If there's some persistent bug that Claude can't seem to figure out how to fix, still run /clear to prevent racking up some giant context drag.\n\nIn your prompt, **give Opus 4.5 a persona**.  e.g. \"You are a senior engineer and award-winning game developer that's renown for building highly performant and addicting games. Build this feature: ...\"  (this is a real one I use, works great).\n\nTaking this a step further - so you don't have a) write this persona out everytime and b) have Claude weigh in on how to improve it even more:  **Create your own custom agent with the /agents slash command**.  I always select \"use claude to help you..\" or whatever it says.  I enter a description of the persona and it generates the agent specs for me. \n\nChaining these all together, my workflow has become...\n\n**use \\[enter agent name\\] to implement Chunks 1-3 in plan \\[paste plan path\\]. Verify no unintended consequences were created from your changes.** \n\n**/clear**\n\n**use game-dev-agent to implement Chunks 4-6 in plan \\[paste plan path\\]. Verify no unintended consequences were created from your changes.** \n\n**/clear**\n\n...rinse &amp; repeat...\n\nI'm sure I'm just barely scratching the surface here, I'd love to hear what I could be doing better.  Please share your own tips in the comments.",
    "source": "reddit",
    "url": "https://reddit.com/r/ClaudeAI/comments/1q9hoe6/some_tips_for_other_newbs_like_me/",
    "author": "u/bri-_-guy",
    "score": 32,
    "date": "2026-01-10",
    "category": "workflow",
    "metadata": {
      "subreddit": "r/ClaudeAI",
      "num_comments": 6
    },
    "scan": {
      "risk_level": "safe",
      "flags": [],
      "llm_analysis": "RISK_LEVEL: SAFE\nSUMMARY: This appears to be legitimate workflow advice for Claude Code CLI usage with no apparent security risks - it focuses on token efficiency and productivity optimization using built-in features."
    },
    "added_at": "2026-01-11T09:56:11.993667"
  },
  {
    "id": "6133bd9ad9f9",
    "title": "My static analysis toolkit to catch what Claude Code misses",
    "content": "Following my\u00a0[previous post about TODO-driven development](https://www.reddit.com/r/ClaudeAI/comments/1q85tlf/i_feel_like_ive_just_had_a_breakthrough_with_how/), several people asked about the static analysis scripts I mentioned. Here you go:\n\nThe Problem:\n\nWhen you're building a large project with Claude Code, you face a unique challenge:\u00a0the AI generates code faster than you can verify it. Claude is remarkably capable, but it doesn't have perfect memory of your entire codebase. Over time, small inconsistencies creep in:\n\n* A Go struct gains a field, but the TypeScript interface doesn't\n* A database column gets added, but the repository struct is missing it\n* A new API endpoint exists in handlers but isn't documented\n* Tests cover happy paths but miss edge cases for 3 of your 27 implementations\n* Query complexity grows without anyone noticing until production slows down\n\nThis is called\u00a0drift\u00a0- the gradual divergence between what should be true and what actually is.\n\nManual code review doesn't scale when Claude is writing 500+ lines per session. I needed automated verification.\n\nThe Solution: Purpose-Built Static Analysis\n\nOver the past \\~9 weeks, I built 14 CLI tools that analyze my Go/TypeScript codebase. Each tool targets a specific category of drift or risk. Here's a couple of them:\n\n# Type Safety &amp; Contract Drift\n\n**1. api-contract-drift**\u00a0\\- Detects mismatches between Go API response types and TypeScript interfaces\n\n    $ go run ./cmd/api-contract-drift\n    DRIFT DETECTED: UserResponse\n      - MissingInTS: CreatedAt (Go has it, TypeScript doesn't)\n      - TypeMismatch: Balance (Go: decimal.Decimal, TS: number)\n    \n\nThis alone has saved me countless runtime bugs. When Claude adds a field to a Go handler, this tool screams if the frontend types weren't updated.\n\n**2. schema-drift-detector**\u00a0\\- Ensures database schema matches Go struct definitions\n\n* Catches orphan columns (DB has it, Go doesn't)\n* Catches orphan fields (Go has it, DB doesn't)\n* Detects type mismatches (critical!)\n* Flags nullable columns without pointer types in Go\n* Identifies missing foreign key indexes\n\n# Code Quality &amp; Security\n\n**3. code-audit**\u00a0\\- The big one. 30+ individual checks across categories:\n\n* **Security:**\u00a0SQL injection vectors, CSRF protection, rate limit vulnerabilities, credential leaks\n* **Quality:**\u00a0N+1 query detection, transaction boundary verification, error response format validation\n* **Domain-specific:**\u00a0Balance precheck race conditions, order status verification, symbol normalization\n\n&amp;#8203;\n\n    $ go run ./cmd/code-audit --category security --format markdown\n    \n\nI run this in CI. Any critical finding blocks the build.\n\n**4. query-complexity-analyzer**\u00a0\\- Scores SQL queries for performance risk\n\n* JOINs, subqueries, GROUP BY, DISTINCT all add to complexity score\n* Flags queries above threshold (default: 20 points)\n* Detects N+1 patterns and implicit JOINs\n* Catches dynamic WHERE clause construction (SQL injection risk)\n\n# Test Coverage Analysis\n\n**5. implementation-test-coverage**\u00a0\\- My project has 27+ specific implementations. This tool:\n\n* Categorizes tests into 14 types (HTTP Mock, Unit, Error Map, Fuzz, Chaos, etc.)\n* Tracks compliance suite coverage (55 shared tests all specific implementations must pass)\n* Identifies which implementations are missing which test categories\n* Maintains a baseline JSON for regression detection\n\n&amp;#8203;\n\n    implementation_A:     142/140 tests (PASS)\n    implementation_B:     138/140 tests (MISSING: chaos, fuzz)\n    implementation_C:     89/115 tests  (FAIL - below mandatory minimum)\n    \n\nThis visibility transformed how I prioritize test writing.\n\n**6. test-type-distribution**\u00a0\\- Shows test type breakdown across the entire codebase\n\n# Architecture &amp; Dead Code\n\n**7. service-dependency-graph**\u00a0\\- Maps service-to-repository dependencies\n\n* Outputs Mermaid diagrams for visualization\n* Catches circular dependencies\n* Shows which services are becoming \"god objects\"\n\n**8. unused-repository-methods**\u00a0\\- Finds dead code\n\n* When Claude refactors, old methods sometimes get orphaned\n* This tool finds them before they rot\n\n**9. missing-index-detector**\u00a0\\- Identifies queries that could benefit from indexes\n\n**10. api-endpoint-inventory**\u00a0\\- Catalogs all HTTP routes\n\n* Essential when you need to verify documentation completeness\n\n# Additional Tools\n\n* **code-stats**\u00a0\\- Generates codebase metrics (lines by package, test-to-code ratio)\n* **implementation-consistency**\u00a0\\- Validates consistent implementation across my implementation clients\n* **symbol-conversion-audit**\u00a0\\- Checks symbol normalization consistency\n* **mock-implementation-finder**\u00a0\\- Finds TODO stubs in test files\n\n# Design Principles\n\nEvery tool follows the same pattern:\n\n1. **Multiple output formats:**\u00a0text (human), JSON (CI), markdown (reports)\n2. **CI mode:**\u00a0Returns appropriate exit codes\n3. **Focused scope:**\u00a0Each tool does one thing well\n4. **Fast execution:**\u00a0Most run in &lt;2 seconds\n\nExample structure:\n\n    func main() {\n        format := flag.String(\"format\", \"text\", \"Output format: text, json, markdown\")\n        ciMode := flag.Bool(\"ci\", false, \"CI mode - exit 1 on findings\")\n        \n    // ... parse flags, find project root via go.mod, run analysis\n    }\n    \n\n# How I Use These\n\n**Daily workflow:**\n\n    # Quick health check\n    go run ./cmd/api-contract-drift\n    go run ./cmd/schema-drift-detector\n    \n    # Before commits\n    go run ./cmd/code-audit --ci\n    \n\n**Weekly deep dive:**\n\n    # Generate reports\n    go run ./cmd/code-stats &gt; docs/reports/stats-$(date +%Y-%m-%d).md\n    go run ./cmd/implementation-test-coverage --format markdown\n    go run ./cmd/query-complexity-analyzer --format markdown\n    \n\n**In CI pipeline:**\n\n* api-contract-drift (blocks on any drift)\n* schema-drift-detector (blocks on type mismatches)\n* code-audit --category security (blocks on critical findings)\n\n# What I Learned\n\n1. **Build tools for YOUR pain points.**\u00a0Generic linters catch generic issues. Your project has domain-specific risks. Build for those.\n2. **JSON output is crucial.**\u00a0It lets you pipe results into other tools, track trends over time, and integrate with CI.\n3. **Fast feedback &gt; perfect analysis.**\u00a0A tool that runs in 1 second gets run constantly. A tool that takes 30 seconds gets skipped.\n4. **Let the tool find the project root.**\u00a0All my tools walk up looking for\u00a0`go.mod`. This means they work from any subdirectory.\n5. **Severity levels matter.**\u00a0Not every finding is equal. Critical blocks CI. Warning gets logged. Info is for reports.\n\n# The Psychological Benefit\n\nJust like my TODO-driven approach, these tools reduce anxiety. I no longer wonder \"did I miss something?\" because I have automated verification running constantly.\n\nClaude is an incredible coding partner, but trust needs verification. These tools are my verification layer. It also saves me a lot of tokens - I saw Claude doing the same bash searches over and over again, and each search takes about 5 to 10 seconds between one search -&gt; \"thinking\" -&gt; the next search. This wastes time and tokens. Now I just run my scripts and tell Claude which files to specifically target in my next task.\n\nI'm happy to share more details or guided brainstorming on how to determine which tools you need based on your unique codebase/project. If there's interest, I could write up another post focusing on this.\n\nWhat static analysis have you found valuable for your AI-assisted development? I'm always looking to add new checks.",
    "source": "reddit",
    "url": "https://reddit.com/r/ClaudeAI/comments/1q9tlaf/my_static_analysis_toolkit_to_catch_what_claude/",
    "author": "u/wynwyn87",
    "score": 24,
    "date": "2026-01-11",
    "category": "workflow",
    "metadata": {
      "subreddit": "r/ClaudeAI",
      "num_comments": 5
    },
    "scan": {
      "risk_level": "warning",
      "flags": [
        "\u26a1 External URL",
        "\u26a1 Command substitution"
      ],
      "llm_analysis": "RISK_LEVEL: WARNING\n\nSUMMARY: Tool suite appears legitimate but lacks source code verification - the SQL injection detection and credential leak scanning could be bypassed or contain blind spots, and running unverified static analysis tools in CI pipelines introduces supply chain risks."
    },
    "added_at": "2026-01-11T09:56:11.998680"
  },
  {
    "id": "519777d17b2b",
    "title": "Claude Code + macbook makes don't even care anymore",
    "content": "I'm a software engineer who spent years as a DevOps guy, so I know Google Cloud and AWS probably better than some of their own employees at this point. But honestly? I don't care anymore. For my personal projects I just spawn Claude with access to a local Bun server and send requests to it. It's ridiculous how well it works.\n\nMy MacBook's CPU is so good and having Claude able to monitor things has made me genuinely lazy about infrastructure. The thought of spawning machines, SSH-ing into them, and setting everything up from scratch just doesn't appeal to me anymore. I've got 14 background CPU-heavy pipeline tasks running locally and it handles them fine.\n\nSo here's what's confusing me. Everyone praises Daytona and these AI-focused sandboxes like crazy. Theo's always going on about how great they are. But honestly I don't get the value at all. Am I missing something or have I just accidentally solved the problem they're trying to solve?\n\nTo be clear, this is all personal project stuff, not production work. Claude Code basically acts as a watcher for my local server pipeline. It monitors everything and warns me if something's running wrong. Combined with my Mac's raw compute power, it just... works. I don't need cloud infrastructure for this.\n\nOP note: asked to claude rewrite it lol \u2764\ufe0f",
    "source": "reddit",
    "url": "https://reddit.com/r/ClaudeAI/comments/1q9sayh/claude_code_macbook_makes_dont_even_care_anymore/",
    "author": "u/Specialist_Farm_5752",
    "score": 51,
    "date": "2026-01-11",
    "category": "workflow",
    "metadata": {
      "subreddit": "r/ClaudeAI",
      "num_comments": 67
    },
    "scan": {
      "risk_level": "warning",
      "flags": [],
      "llm_analysis": "RISK_LEVEL: WARNING\n\nSUMMARY: Describes giving Claude local server access and system monitoring capabilities without mentioning authentication, access controls, or security boundaries, which could enable unauthorized system access or privilege escalation if Claude is compromised or manipulated."
    },
    "added_at": "2026-01-11T09:56:11.999177"
  },
  {
    "id": "a516fbed5329",
    "title": "A Vision for a Claude Code IDE",
    "content": "\\*\\*Edit\\*\\*: Not sure if you can actually see the video on reddit so here's the youtube link: [https://youtu.be/YzfDog-tRmo?si=c2tUgR24vjRter2M](https://youtu.be/YzfDog-tRmo?si=c2tUgR24vjRter2M)\n\n\n\nI've been using Claude Code constantly and it's become one of the most powerful tools in my workflow. But I'm not a terminal person. I like seeing my files in a tree. I want visual feedback.\n\nSo over the past few weeks, I started designing what a dedicated Claude Code IDE might look like, not a VS Code extension, but a purpose-built interface that treats Claude as a first-class collaborator.\n\nI made a video walkthrough and a live demo you can play with. Some highlights:\n\n**Context Graph**: A visual way to see and edit everything Claude knows. Your preferences, org standards, project context. When Claude's referencing something out of date, you can just fix it instead of wrestling with prompts.\n\n**Interview Mode**: Claude asks clarifying questions before diving in. Saves hours of reworking.\n\n**Skill Preservation**: This one was inspired by some of Anthropic's own research I was reading where they mentiond their own engineers were worried about skill atrophy. I think this is an important feature not just for coders but whoever might be using this for knowledge work. You can tell Claude which skills you want to keep sharp, and sometimes it'll suggest you write that part manually, just enough to keep the muscle memory alive.\n\n**Live Annotations**: For people building with AI who don't fully understand every tool they're using, or really by extension for anything where Claude needs to refer to something on the screen. Claude can walk you through things like source control with interactive on-screen annotations.\n\n**Workflows**: Visual node-based workflows that you can build or have Claude build for you. Code reviews, security audits, whatever you do repeatedly. I'm imagining this would be a great way to use their Agents SDK or have claude connect the parts for you so you can build the backend for a user-facing agent, stuff like that.\n\n**Profile**: A meta layer where Claude reflects on your week, tracks skills you're developing, and helps you see your own trajectory. Not just \"what did I ship\" but \"how am I growing.\"\n\nI tried to think through the whole user experience, not just bolt on features. The design language is warm (Anthropic's earthy tones) with a signature \"notched container\" element that nods to the terminal origins.\n\nCurious what you all think. What's missing? What would you want in a Claude Code IDE? I know a lot of people super love the terminal but tbh I've just always worked in an IDE and that's how I prefer to work (and the people who love using terminals should be able to keep working that way of course). I also think that\n\nLive demo: [https://claudecodeide.vercel.app/](https://claudecodeide.vercel.app/)\n\nBlog post with more detail: [https://www.justinwetch.com/blog/claudecodeide](https://www.justinwetch.com/blog/claudecodeide)\n\nThank you for your time and checking this out! Built with claude btw ;-)",
    "source": "reddit",
    "url": "https://reddit.com/r/ClaudeAI/comments/1q9fqiq/a_vision_for_a_claude_code_ide/",
    "author": "u/JustinWetch",
    "score": 47,
    "date": "2026-01-10",
    "category": "workflow",
    "metadata": {
      "subreddit": "r/ClaudeAI",
      "num_comments": 21
    },
    "scan": {
      "risk_level": "warning",
      "flags": [
        "\u26a1 External URL"
      ],
      "llm_analysis": "RISK_LEVEL: WARNING\nSUMMARY: Contains external links to YouTube, Vercel demo, and personal blog that could potentially host malicious content or be used for phishing, though the content itself appears to be a legitimate IDE concept presentation."
    },
    "added_at": "2026-01-11T09:56:11.999794"
  },
  {
    "id": "75c22649e400",
    "title": "I built an agent to triage production alerts",
    "content": "Hey folks,\n\nI just coded an AI on-call engineer that takes raw production alerts, reasons with context and past incidents, decides whether to auto-handle or escalate, and wakes humans up only when it actually matters.\n\nWhen an alert comes in, the agent reasons about it in context and decides whether it can be handled safely or should be escalated to a human.\n\nThe flow looks like this:\n\n* An API endpoint receives alert messages from monitoring systems\n* A durable agent workflow kicks off\n* LLM reasons about risk and confidence\n* Agent returns Handled or Escalate\n* Every step is fully observable\n\nWhat I found interesting is that the agent gets better over time as it sees repeated incidents. Similar alerts stop being treated as brand-new problems, which cuts down on noise and unnecessary escalations.\n\nThe whole thing runs as a durable workflow with step-by-step tracking, so it\u2019s easy to see how each decision was made and why an alert was escalated (or not).\n\nThe project is intentionally focused on the triage layer, not full auto-remediation. Humans stay in the loop, but they\u2019re pulled in later, with more context.\n\nIf you want to see it in action, I put together a full walkthrough\u00a0[here](https://www.tensorlake.ai/blog/building-outage-agent).\n\nAnd the code is up here if you\u2019d like to try it or extend it:\u00a0[GitHub Repo](https://github.com/tensorlakeai/examples/tree/main/outage-agent)\n\nWould love feedback from you if you have built similar alerting systems.",
    "source": "reddit",
    "url": "https://reddit.com/r/ChatGPTCoding/comments/1q7k6lz/i_built_an_agent_to_triage_production_alerts/",
    "author": "u/Arindam_200",
    "score": 20,
    "date": "2026-01-08",
    "category": "workflow",
    "metadata": {
      "subreddit": "r/ChatGPTCoding",
      "num_comments": 3
    },
    "scan": {
      "risk_level": "warning",
      "flags": [
        "\u26a1 External URL"
      ],
      "llm_analysis": "RISK_LEVEL: WARNING\n\nSUMMARY: Legitimate project with security concerns around AI-based alert triage having potential for false negatives that could suppress critical alerts, plus typical risks of automated systems handling production security events without sufficient human oversight."
    },
    "added_at": "2026-01-11T09:56:12.000345"
  }
]